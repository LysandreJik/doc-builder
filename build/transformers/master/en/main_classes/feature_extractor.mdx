---
local: feature-extractor
sections:
- local: None
  title: FeatureExtractionMixin
- local: transformers.SequenceFeatureExtractor
  title: SequenceFeatureExtractor
- local: transformers.BatchFeature
  title: BatchFeature
- local: transformers.ImageFeatureExtractionMixin
  title: ImageFeatureExtractionMixin
title: Feature Extractor
---
<script>
import Tip from "./Tip.svelte";
import Youtube from "./Youtube.svelte";
import Docstring from "./Docstring.svelte";
import CodeBlock from "./CodeBlock.svelte";
import CodeBlockFw from "./CodeBlockFw.svelte";
import IconCopyLink from "./IconCopyLink.svelte";

export let fw: "pt" | "tf"
</script>

<!--Copyright 2021 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

<h1 id="feature-extractor">Feature Extractor</h1>

A feature extractor is in charge of preparing input features for a multi-modal model. This includes feature extraction
from sequences, *e.g.*, pre-processing audio files to Log-Mel Spectrogram features, feature extraction from images
*e.g.* cropping image image files, but also padding, normalization, and conversion to Numpy, PyTorch, and TensorFlow
tensors.


<h2 id="None">FeatureExtractionMixin</h2>

<div class="docstring">

<docstring><name>"class transformers.feature\_extraction\_utils.FeatureExtractionMixin"</name><anchor>"transformers.feature_extraction_utils.FeatureExtractionMixin"</anchor><source>"https://github.com/huggingface/transformers/blob/master/src/transformers/feature_extraction_utils.py#L198"</source><parameters>[{"name": "**kwargs", "val": ""}]</parameters></docstring>

    This is a feature extraction mixin used to provide saving/loading functionality for sequential and image feature
    extractors.
    

</div>

<h2 id="transformers.SequenceFeatureExtractor">SequenceFeatureExtractor</h2>

<div class="docstring">

<docstring><name>"class transformers.SequenceFeatureExtractor"</name><anchor>"transformers.SequenceFeatureExtractor"</anchor><source>"https://github.com/huggingface/transformers/blob/master/src/transformers/feature_extraction_sequence_utils.py#L38"</source><parameters>[{"name": "feature_size", "val": ": int"}, {"name": "sampling_rate", "val": ": int"}, {"name": "padding_value", "val": ": float"}, {"name": "**kwargs", "val": ""}]</parameters><paramsdesc>[{"name": "feature_size", "description": "- **feature_size** (`int`) -- The feature dimension of the extracted features.", "anchor": "transformers.SequenceFeatureExtractor.feature_size"}, {"name": "sampling_rate", "description": "- **sampling_rate** (`int`) -- The sampling rate at which the audio files should be digitalized expressed in Hertz per second (Hz).", "anchor": "transformers.SequenceFeatureExtractor.sampling_rate"}, {"name": "padding_value", "description": "- **padding_value** (`float`) -- The value that is used to fill the padding values / vectors.", "anchor": "transformers.SequenceFeatureExtractor.padding_value"}]</paramsdesc></docstring>

This is a general feature extraction class for speech recognition.





<div class="docstring">
<docstring><name>"pad"</name><anchor>"transformers.SequenceFeatureExtractor.pad"</anchor><source>"https://github.com/huggingface/transformers/blob/master/src/transformers/feature_extraction_sequence_utils.py#L61"</source><parameters>[{"name": "processed_features", "val": ": typing.Union[transformers.feature_extraction_utils.BatchFeature, typing.List[transformers.feature_extraction_utils.BatchFeature], typing.Dict[str, transformers.feature_extraction_utils.BatchFeature], typing.Dict[str, typing.List[transformers.feature_extraction_utils.BatchFeature]], typing.List[typing.Dict[str, transformers.feature_extraction_utils.BatchFeature]]]"}, {"name": "padding", "val": ": typing.Union[bool, str, transformers.file_utils.PaddingStrategy] = True"}, {"name": "max_length", "val": ": typing.Optional[int] = None"}, {"name": "truncation", "val": ": bool = False"}, {"name": "pad_to_multiple_of", "val": ": typing.Optional[int] = None"}, {"name": "return_attention_mask", "val": ": typing.Optional[bool] = None"}, {"name": "return_tensors", "val": ": typing.Union[str, transformers.file_utils.TensorType, NoneType] = None"}]</parameters><paramsdesc>[{"name": "processed_features", "description": "- **processed_features** ([BatchFeature](/docs/transformers/master/en/main_classes/feature_extractor#transformers.BatchFeature), list of [BatchFeature](/docs/transformers/master/en/main_classes/feature_extractor#transformers.BatchFeature), `Dict[str, List[float]]`, `Dict[str, List[List[float]]` or `List[Dict[str, List[float]]]`) -- Processed inputs. Can represent one input ([BatchFeature](/docs/transformers/master/en/main_classes/feature_extractor#transformers.BatchFeature) or `Dict[str, List[float]]`) or a batch of input values / vectors (list of [BatchFeature](/docs/transformers/master/en/main_classes/feature_extractor#transformers.BatchFeature), _Dict[str, List[List[float]]]_ or _List[Dict[str, List[float]]]_) so you can use this method during preprocessing as well as in a PyTorch Dataloader collate function. Instead of `List[float]` you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors), see the note above for the return type.", "anchor": "transformers.SequenceFeatureExtractor.pad.processed_features"}, {"name": "padding", "description": "- **padding** (`bool`, `str` or [PaddingStrategy](/docs/transformers/master/en/internal/file_utils#transformers.file_utils.PaddingStrategy), _optional_, defaults to `True`) -- Select a strategy to pad the returned sequences (according to the model's padding side and padding index) among: - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single sequence if provided). - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum acceptable input length for the model if that argument is not provided. - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different lengths).", "anchor": "transformers.SequenceFeatureExtractor.pad.padding"}, {"name": "max_length", "description": "- **max_length** (`int`, _optional_) -- Maximum length of the returned list and optionally padding length (see above).", "anchor": "transformers.SequenceFeatureExtractor.pad.max_length"}, {"name": "truncation", "description": "- **truncation** (`bool`) -- Activates truncation to cut input sequences longer than `max_length` to `max_length`.", "anchor": "transformers.SequenceFeatureExtractor.pad.truncation"}, {"name": "pad_to_multiple_of", "description": "- **pad_to_multiple_of** (`int`, _optional_) -- If set will pad the sequence to a multiple of the provided value. This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128.", "anchor": "transformers.SequenceFeatureExtractor.pad.pad_to_multiple_of"}, {"name": "return_attention_mask", "description": "- **return_attention_mask** (`bool`, _optional_) -- Whether to return the attention mask. If left to the default, will return the attention mask according to the specific feature_extractor's default. [What are attention masks?](../glossary#attention-mask)", "anchor": "transformers.SequenceFeatureExtractor.pad.return_attention_mask"}, {"name": "return_tensors", "description": "- **return_tensors** (`str` or [TensorType](/docs/transformers/master/en/internal/file_utils#transformers.TensorType), _optional_) -- If set, will return tensors instead of list of python integers. Acceptable values are: - `'tf'`: Return TensorFlow `tf.constant` objects. - `'pt'`: Return PyTorch `torch.Tensor` objects. - `'np'`: Return Numpy `np.ndarray` objects.", "anchor": "transformers.SequenceFeatureExtractor.pad.return_tensors"}]</paramsdesc></docstring>

Pad input values / input vectors or a batch of input values / input vectors up to predefined length or to the
max sequence length in the batch.

Padding side (left/right) padding values are defined at the feature extractor level (with
`self.padding_side`, `self.padding_value`)

<Tip>

If the `processed_features` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors,
the result will use the same type unless you provide a different tensor type with `return_tensors`. In
the case of PyTorch tensors, you will lose the specific device of your tensors however.

</Tip>




</div></div>

<h2 id="transformers.BatchFeature">BatchFeature</h2>

<div class="docstring">

<docstring><name>"class transformers.BatchFeature"</name><anchor>"transformers.BatchFeature"</anchor><source>"https://github.com/huggingface/transformers/blob/master/src/transformers/feature_extraction_utils.py#L55"</source><parameters>[{"name": "data", "val": ": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"}, {"name": "tensor_type", "val": ": typing.Union[NoneType, str, transformers.file_utils.TensorType] = None"}]</parameters><paramsdesc>[{"name": "data", "description": "- **data** (`dict`) -- Dictionary of lists/arrays/tensors returned by the __call__/pad methods ('input_values', 'attention_mask', etc.).", "anchor": "transformers.BatchFeature.data"}, {"name": "tensor_type", "description": "- **tensor_type** (`Union[None, str, TensorType]`, _optional_) -- You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at initialization.", "anchor": "transformers.BatchFeature.tensor_type"}]</paramsdesc></docstring>

Holds the output of the [pad()](/docs/transformers/master/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad) and feature extractor specific
`__call__` methods.

This class is derived from a python dictionary and can be used as a dictionary.





<div class="docstring">
<docstring><name>"convert\_to\_tensors"</name><anchor>"transformers.BatchFeature.convert_to_tensors"</anchor><source>"https://github.com/huggingface/transformers/blob/master/src/transformers/feature_extraction_utils.py#L110"</source><parameters>[{"name": "tensor_type", "val": ": typing.Union[str, transformers.file_utils.TensorType, NoneType] = None"}]</parameters><paramsdesc>[{"name": "tensor_type", "description": "- **tensor_type** (`str` or [TensorType](/docs/transformers/master/en/internal/file_utils#transformers.TensorType), _optional_) -- The type of tensors to use. If `str`, should be one of the values of the enum [TensorType](/docs/transformers/master/en/internal/file_utils#transformers.TensorType). If `None`, no modification is done.", "anchor": "transformers.BatchFeature.convert_to_tensors.tensor_type"}]</paramsdesc></docstring>

Convert the inner content to tensors.




</div>
<div class="docstring">
<docstring><name>"to"</name><anchor>"transformers.BatchFeature.to"</anchor><source>"https://github.com/huggingface/transformers/blob/master/src/transformers/feature_extraction_utils.py#L175"</source><parameters>[{"name": "device", "val": ": typing.Union[str, ForwardRef('torch.device')]"}]</parameters><paramsdesc>[{"name": "device", "description": "- **device** (`str` or `torch.device`) -- The device to put the tensors on.", "anchor": "transformers.BatchFeature.to.device"}]</paramsdesc><rettype>[BatchFeature](/docs/transformers/master/en/main_classes/feature_extractor#transformers.BatchFeature)</rettype><retdesc>The same instance after modification.</retdesc></docstring>

Send all values to device by calling `v.to(device)` (PyTorch only).








</div></div>

<h2 id="transformers.ImageFeatureExtractionMixin">ImageFeatureExtractionMixin</h2>

<div class="docstring">

<docstring><name>"class transformers.ImageFeatureExtractionMixin"</name><anchor>"transformers.ImageFeatureExtractionMixin"</anchor><source>"https://github.com/huggingface/transformers/blob/master/src/transformers/image_utils.py#L76"</source><parameters>[]</parameters></docstring>

    Mixin that contain utilities for preparing image features.
    


<div class="docstring">
<docstring><name>"center\_crop"</name><anchor>"transformers.ImageFeatureExtractionMixin.center_crop"</anchor><source>"https://github.com/huggingface/transformers/blob/master/src/transformers/image_utils.py#L211"</source><parameters>[{"name": "image", "val": ""}, {"name": "size", "val": ""}]</parameters><paramsdesc>[{"name": "image", "description": "- **image** (`PIL.Image.Image` or `np.ndarray` or `torch.Tensor`) -- The image to resize.", "anchor": "transformers.ImageFeatureExtractionMixin.center_crop.image"}, {"name": "size", "description": "- **size** (`int` or `Tuple[int, int]`) -- The size to which crop the image.", "anchor": "transformers.ImageFeatureExtractionMixin.center_crop.size"}]</paramsdesc></docstring>

Crops `image` to the given size using a center crop. Note that if the image is too small to be cropped to
the size given, it will be padded (so the returned result has the size asked).




</div>
<div class="docstring">
<docstring><name>"normalize"</name><anchor>"transformers.ImageFeatureExtractionMixin.normalize"</anchor><source>"https://github.com/huggingface/transformers/blob/master/src/transformers/image_utils.py#L152"</source><parameters>[{"name": "image", "val": ""}, {"name": "mean", "val": ""}, {"name": "std", "val": ""}]</parameters><paramsdesc>[{"name": "image", "description": "- **image** (`PIL.Image.Image` or `np.ndarray` or `torch.Tensor`) -- The image to normalize.", "anchor": "transformers.ImageFeatureExtractionMixin.normalize.image"}, {"name": "mean", "description": "- **mean** (`List[float]` or `np.ndarray` or `torch.Tensor`) -- The mean (per channel) to use for normalization.", "anchor": "transformers.ImageFeatureExtractionMixin.normalize.mean"}, {"name": "std", "description": "- **std** (`List[float]` or `np.ndarray` or `torch.Tensor`) -- The standard deviation (per channel) to use for normalization.", "anchor": "transformers.ImageFeatureExtractionMixin.normalize.std"}]</paramsdesc></docstring>

Normalizes `image` with `mean` and `std`. Note that this will trigger a conversion of
`image` to a NumPy array if it's a PIL Image.




</div>
<div class="docstring">
<docstring><name>"resize"</name><anchor>"transformers.ImageFeatureExtractionMixin.resize"</anchor><source>"https://github.com/huggingface/transformers/blob/master/src/transformers/image_utils.py#L188"</source><parameters>[{"name": "image", "val": ""}, {"name": "size", "val": ""}, {"name": "resample", "val": " = 2"}]</parameters><paramsdesc>[{"name": "image", "description": "- **image** (`PIL.Image.Image` or `np.ndarray` or `torch.Tensor`) -- The image to resize.", "anchor": "transformers.ImageFeatureExtractionMixin.resize.image"}, {"name": "size", "description": "- **size** (`int` or `Tuple[int, int]`) -- The size to use for resizing the image.", "anchor": "transformers.ImageFeatureExtractionMixin.resize.size"}, {"name": "resample", "description": "- **resample** (`int`, _optional_, defaults to `PIL.Image.BILINEAR`) -- The filter to user for resampling.", "anchor": "transformers.ImageFeatureExtractionMixin.resize.resample"}]</paramsdesc></docstring>

Resizes `image`. Note that this will trigger a conversion of `image` to a PIL Image.




</div>
<div class="docstring">
<docstring><name>"to\_numpy\_array"</name><anchor>"transformers.ImageFeatureExtractionMixin.to_numpy_array"</anchor><source>"https://github.com/huggingface/transformers/blob/master/src/transformers/image_utils.py#L118"</source><parameters>[{"name": "image", "val": ""}, {"name": "rescale", "val": " = None"}, {"name": "channel_first", "val": " = True"}]</parameters><paramsdesc>[{"name": "image", "description": "- **image** (`PIL.Image.Image` or `np.ndarray` or `torch.Tensor`) -- The image to convert to a NumPy array.", "anchor": "transformers.ImageFeatureExtractionMixin.to_numpy_array.image"}, {"name": "rescale", "description": "- **rescale** (`bool`, _optional_) -- Whether or not to apply the scaling factor (to make pixel values floats between 0. and 1.). Will default to `True` if the image is a PIL Image or an array/tensor of integers, `False` otherwise.", "anchor": "transformers.ImageFeatureExtractionMixin.to_numpy_array.rescale"}, {"name": "channel_first", "description": "- **channel_first** (`bool`, _optional_, defaults to `True`) -- Whether or not to permute the dimensions of the image to put the channel dimension first.", "anchor": "transformers.ImageFeatureExtractionMixin.to_numpy_array.channel_first"}]</paramsdesc></docstring>

Converts `image` to a numpy array. Optionally rescales it and puts the channel dimension as the first
dimension.




</div>
<div class="docstring">
<docstring><name>"to\_pil\_image"</name><anchor>"transformers.ImageFeatureExtractionMixin.to_pil_image"</anchor><source>"https://github.com/huggingface/transformers/blob/master/src/transformers/image_utils.py#L88"</source><parameters>[{"name": "image", "val": ""}, {"name": "rescale", "val": " = None"}]</parameters><paramsdesc>[{"name": "image", "description": "- **image** (`PIL.Image.Image` or `numpy.ndarray` or `torch.Tensor`) -- The image to convert to the PIL Image format.", "anchor": "transformers.ImageFeatureExtractionMixin.to_pil_image.image"}, {"name": "rescale", "description": "- **rescale** (`bool`, _optional_) -- Whether or not to apply the scaling factor (to make pixel values integers between 0 and 255). Will default to `True` if the image type is a floating type, `False` otherwise.", "anchor": "transformers.ImageFeatureExtractionMixin.to_pil_image.rescale"}]</paramsdesc></docstring>

Converts `image` to a PIL Image. Optionally rescales it and puts the channel dimension back as the last
axis if needed.




</div></div>
